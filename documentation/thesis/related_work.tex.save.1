% ---------------------------------------------------------
\chapter{Background and Related Work}
\label{b_rel_work}
% ---------------------------------------------------------

This chapter provides an overview of the general topics of this thesis. 
In section~\ref{background} an introduction about variability models, performance assessment and performance prediction is given. 
Related work and approaches that analyse performance similar to our approach is summarized in section~\ref{rel_work}.

\section{Background}
\label{background}

The background section provides an introduction about necessary fundamentals. 
We give an overview about configurable software systems~(section~\ref{background_conf_sys}) and variability models~(section~\ref{background_variab_models}). 
We define the term performance~(section~\ref{background_perf}) for this thesis, describe how performance of software can be measured and how to learn models to be able to predict performance.

\subsection{Configurable Software Systems}
\label{background_conf_sys}
% Problem Space (Whole space) - Constraints (illegal feature combinations), default settings, optimizations, ... -> Solution Space ()
% variability models of configurable software systems
Today, almost every software system provide different options to configure them. 
Every single combination of configurations results in a \textit{variant} of the program, similar to \acp{SPL}\footnote{``A software product line (SPL) is a set of software-intensive systems that share a common, managed set of features satisfying the specific needs of a particular market segment or mission and that are developed from a common set of core assets in a prescribed way.''~\cite{northrop2010spl}}~\cite{siegmund2012spl}. 
Each variant comprises the set of activated configuration options also referred to as \textit{features}. 
The combination of features describe the user desired characteristics of programs~\cite{czarnecki2000generative}. 
Imagine a database engine. 
The selection (or rejection) of features like compression, encryption or the use of a specific indexing strategy results in a number of different variants, that fulfil different requirements desired by users. 
Requirements that describe what a system can do are called functional properties~\cite{siegmund2012spl,guo2013variability}. 
In addition to the functionality requirements of software, there are also non-functional properties to be met~\cite{siegmund2012spl,guo2013variability,sarkar2015cost,siegmund2015performance}. 
In literature there exist different definitions of non-functional properties. 
Glinz surveyed the existing definitions in 2007~\cite{glinz2007non}, e.g. ``The required overall attributes of the system, including portability, reliability, efficiency, human engineering, testability, understandability, and modifiability''~\cite{davis1993software} or ``Requirements which are not specifically concerned with the functionality of a system. 
They place restrictions on the product being developed and the development process, and they specify external constraints that the product must meet''~\cite{kotonya1998requirements}. 
There are more definitions of non-functional properties but during this thesis we want to stick to the definition of Robertson and Robertson from 1999 because these definition seems to be wildly used since then~\cite{nuseibeh2000requirements,jackson2001problem,cohn2004user,siegmund2012spl}: ``A property, or quality, that the product must have, such as an appearance, or a speed or accuracy property''~\cite{robertson1999mastering}.

\subsection{Variability Models}
\label{background_variab_models}

After defining configurable software systems with their functional and non-functional properties we want to show how to model them. 
\textit{Feature models} define a common way to describe all valid configurations of software systems~\cite{siegmund2015performance,sarkar2015cost,guo2013variability}. 
They were introduced in 1990 by Kang et al.~\cite{kang1990feature}. 
They became an important instrument for modelling variability in software product lines. 
With feature models every configuration option can be expressed as a feature. 
There are two different representations of feature models: textual notations and graphical representations. 
Feature models in textual from (like SXFM~\cite{Mendonca2009SSP} and Velvet~\cite{rosenmuller2011multi}) have been developed to construct and maintain models of systems with a huge amount of features which might be presented poorly with a graphical tool. 
Whereas feature models with a graphical representation (also called feature diagrams~\cite{kang1990feature}) provide the possibility to create and manipulate feature models through a graphical user interface. 
Furthermore, feature diagrams are widely used in scientific literature to present analysed systems. 


Feature diagrams build up a tree-structured graph, as seen in the example diagram shown in figure~\ref{background_variab_models_feature_diagram_example}. 
With this example diagram we will describe the elements of feature diagrams. 
The root node of the tree \textsf{Database\_Engine} represent the whole database model. 
All nodes in this tree represent either \textit{abstract} of \textit{concrete} features. 
Abstract features like \textsf{Compression} and \textsf{OS} are used to structure the feature model whereas concrete features represent functionality. 
There are two nodes of the database engine modelled as \textit{mandatory}. 
This nodes have to be selected in each valid configuration because they might implement some basic functionality which has to be always available. 
Optional features like \textsf{Indexing} or \textsf{Compression} can be selected to form a valid variant. 
This database for example need to have an operating system specified and can use compression algorithms. 
There are two more elements available in feature diagrams: \textit{or}- and \textit{alternative}. 
The alternative represent the logical XOR, so only one child can be selected at a time. 
Whereas the or enables the selection of one or more children. 
If \textsf{Encryption} is selected in this example then either \textsf{AES} or \textsf{RSA} or both must also be selected. 
In case of the mandatory abstract feature \textsf{OS} which itself only groups available operation systems one has to select either \textsf{Unix} or \textsf{Windows}. 
The structure of the tree, including the parent-children relationship and the different elements, define a set of constraints for selecting valid configurations. 
In addition to that there are so called \textit{cross-tree constraints} which can be model through a textual representation. 
For example if some user want to have zip-compression enabled he also has to stick to Windows. 
These constraints restrict the amount of valid configurations. 
In total there are 64 possible configurations which are valid. 
24 if \textsf{Windows} is initially selected and 40 if the initial selection is \textsf{Unix}. 
We used FeatureIDE~\cite{kastner2009featureide} to model this example. 
The textual representation of the feature model is available in the appendix~\ref{app_feature_diagrams}.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{images/example_database}
  \caption{Example database model (extended form of~\ref{background_conf_sys}).}
  \label{background_variab_models_feature_diagram_example}
\end{figure}

\subsection{Performance Assessment}
\label{background_perf}

The last sections cover the introduction to Configurable Software Systems~(section~\ref{background_conf_sys}) and their representation through models~(section~\ref{background_variab_models}). 
This section treats with the term performance of configurable software systems. 
We want to define performance for this thesis~(section~\ref{background_perf}) and show how to assess performance to learn models from tested configurations. 
After that we present approaches of how these models then can predict performance.

\subsubsection{Performance in General}
\label{perf_general}

Performance is an important quality of software systems. 
As scribed in the introduction (\ref{chap:introduction}) definitions of performance differ dependent on the perspective. 
The most used way is to explain performance of software as a non-functional property~\cite{Molyneaux:2009:AAP:1550832,liggesmeyer2002software}. 
They describe how a system provides its functionality. 
In contrast to that, there are functional properties, which describe what a system should do. 
To decide if a system performs well, non-functional properties are compared to so called \acp{NFR}. 
They constitute a set of requirements which the software should fulfil. 
\acp{NFR} should be defined before software is tested. 
The set of requirements manifold. 
Each type of software system has different metrics which should be analysed. 
A web server for example can be analysed through the following metrics:

\begin{description}
	\item [Hit Rate] is the total number of requests arriving at a server in a defined time frame.
	\item [Latency] describes the time that has passed from the request message has started being sent until the response message starts being received.
	\item [Response Time] describes the time that has passed from the request message has started being sent until the whole response message is received.
	\item [Availability] describes the time that the software is available to the user.
\end{description}

With client-server applications communication take place through networks, whereas with off-line applications the focus during the analysis of performance metrics lies on the intrinsic properties.
Off-line applications usually run on a single device and do not need to communicate direct with other systems. 
The following are some performance metrics of off-line applications:

\begin{description}
	\item [Resource Utilization] describes the fraction to which a system uses its resources\footnote{A resource is a system element that offers some service to other system elements that require them. Resources can be categorized into hardware components(CPU, bus, storage), logical elements(buffers, locks, semaphores) and processing resources (processes, threads)\cite{woodside2007future}.}.
	\item [Response Time] describes the time that is needed to finish an operation.
	\item [Throughput] is the amount of work done in a defined time.
	\item [Availability] describes the time that the software is available to the user.
	\item [Scalability] describes the ability to grow and perform a increasing amount of work. 
\end{description}

For this thesis we will focus on off-line systems because we do not want to cope with the measurement inaccuracy which might accompany with the hardware and software of networks resources. 
Furthermore we want to define performance as the amount of work done per unit of time~\ref{def:perf1} like~\cite{tsirogiannis2010analyzing}. 
So performance can be defines as:
\begin{equation}
	\label{def:perf1}
	\mbox{Performance}=\frac{\mbox{Work Done}}{\mbox{Execution Time}}
\end{equation}

Because we want to analyse performance of configurable software systems we execute different variants of them using the same inputs (Benchmarks\footnote{Benchmarking is the process of running software with standardized tests to be able to compare the results and assess their relative performance.}) for all configurations. 
Thereby, we are able to compare them and because the word done is always the same, we are able to concretize~\ref{def:perf2} our formula like~\cite{siegmund2015performance}:

%Because we want to analyze performance of software by executing benchmarks, the work done is always the same. So for this thesis we can define performance like~\cite{siegmund2015performance}:
\begin{equation}
	\label{def:perf2}
	\mbox{Performance}=\mbox{Execution Time}
\end{equation}

To identify performance hot-spots we also acquire the run-time of each method which is executed.
We are interested in the time spent in each method (net-time).
The concrete approach is explained in section~\ref{perf_measure}. 


\subsubsection{\ac{SPE}}

After defining performance in general we go on outlining the process of analysing performance of software systems.
The process of analysing performance of software systems is defined as follows:
\begin{quote}
``\ac{SPE} represents the entire collection of software engineering activities and related analyses used throughout the software development cycle, which are directed to meeting performance requirements.''~\cite{woodside2007future}.
\end{quote}
There are two general approaches in literature~\cite{woodside2007future}: \textit{measurement-based} and \textit{model-based}. 
The model-based approach is applied early in the development cycle. 
It tries to create performance models to adjust design and architecture as early as possible. 
The measurement-base approach is applied late in the development cycle, because it will run tests and diagnosis to tune the software.
The whole process of \ac{SPE} activities can be structured into six parts. 
The first one is to identify the \ac{NFR} which should be analysed. In section~\ref{perf_general} we sum up the important ones. 
Defining and analysing the requirements is the second step~\cite{woodside2007future}. 
This includes to specify the environment in which the software run. 
For the sake of reproducibility the environment should be defined carefully.  
To select specific tasks, there are many different benchmarks available (\href{https://bapco.com/}{BAPCo}, \href{https://www.eembc.org/}{EEMBC}, \href{http://www.spec.org/}{SPEC}). 
These and other benchmarks provide the possibility to compare one software product to another with a set of standardized tasks and workloads. 
Another possibility is to use benchmarks which are provided by the software itself. 
This may enable analysis of all functionality of software if the provided benchmarks are selected carefully. 
The third activity is to design models which predict performance from detailed design, architecture and usage scenarios. 
These models describe how systems use resources and how resource contention affects operation. 
These models are able to predict system properties before they are implemented to gather knowledge as early as possible in the software development live cycle.
The next \ac{SPE} activity is performance testing. 
Tests can cover only parts or the entire software, dependent on the selected benchmarks. 
Here we get actual values for the performance metrics we selected.
From the gathered data we can then learn models to predict the effect of changes to the system, which is the fifth step of \ac{SPE}. 
A lot of effort has been spent in the last years to understand an improve performance. 
Section~\ref{rel_perf_pred} summarizes more recent measurement-based performance prediction approaches which emphasize variability.
The last step is to analyse the total system. 
Here the final deployed system is analysed regarding to the defined performance goals.

This thesis identifies concerns, defines and analyses requirements, conducts performance tests and learns models for performance prediction of configurable software systems. It also evaluates 


\section{Related Work}
\label{rel_work}

While the last section describes the necessary background for configurable software systems, performance and \ac{SPE}, this section deals with the topics performance prediction and performance analysis of Java projects. 

% Performance in software systems
\subsection{Performance Prediction}
\label{rel_perf_pred}

The task of learning and predicting performance is subject of research for several years. Since then several measurement-based performance prediction models were developed. They all work in general the same. They sample a subset of configurations because measuring performance of each configuration is not practicable if the configuration space is to big. They split the available measurements into two sets (learning set and test set). For learning a model they use the learning set and to be able to assess the accuracy of the model they compare the test set with the prediction of the model. In the following we describe the differences of some models.

On task with \acp{SPL} is to verify the large number of possible products. \cite{thum2012family} presented the approach, called family-based verification, in which the whole \ac{SPL} is encoded in a single \textit{meta-product} which simulates the behaviour of all individual products of the product line. The idea is to verify only the generated meta-product. It contains the implementation and specification of every feature. So it is able to simulate each product. They verify the meta-product (family-based) instead of verifying all products separately (product-based). Because with the meta-product they do not have different products from a product line, but a single system, they use the single-system theorem prover KeY for verification. They adapted the product simulator introduced by~\cite{apel2011detection} to translate the Java-based product line into the meta-product. In comparison to the product-based approach they were able to save more than 85\% of the verification effort.






All aforementioned approaches rely on analysing and evaluating configurable software systems (or \acp{SPL}). 
The approach from \cite{thum2012family} reduces validation time of \acp{SPL} by 85\% for their case study but this approach is only reliable if the configuration space is small enough. 

%General: how can performance be learned?
%- Family-Based Deductive Verification of Software Product Lines (1012)
%- SPLConqueror (2012)
%- Performance-Influence Models for Highly Configurable Systems
%- CART
%- Zhang et al.
%- Discussion: sampling strategies, sample size

% How to find related work

\subsection{Performance of Java Projects}
\label{rel_perf_java}





2007 Statistically Rigorous Java Performance Evaluation
2015 Tracking Down Performance Variation against Source Code Evolution

